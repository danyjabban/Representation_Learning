{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-18T02:09:51.452797Z",
     "start_time": "2023-09-18T02:09:51.407861Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mse=14.499999999999998\n",
      "dL/dW1, res before re-shaping\n",
      " \\begin{bmatrix}\n",
      "  0.00 &  0.00 &  13.00 & -1.00 &  26.00 & -2.00\n",
      "\\end{bmatrix}\n",
      "res after re-shaping\n",
      " \\begin{bmatrix}\n",
      "  0.00 &  0.00\\\\\n",
      "  13.00 & -1.00\\\\\n",
      "  26.00 & -2.00\n",
      "\\end{bmatrix}\n",
      "tensor(14.5000, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "relu dL/dW1\n",
      " tensor([[ 0.0000,  0.0000],\n",
      "        [13.0000, -1.0000],\n",
      "        [26.0000, -2.0000]], dtype=torch.float64) \n",
      "\n",
      "mse=14.499999999999998\n",
      "dL/dW2, res before re-shaping\n",
      " \\begin{bmatrix}\n",
      " -4.00 & -10.00 & -4.00 & -10.00\n",
      "\\end{bmatrix}\n",
      "res after re-shaping\n",
      " \\begin{bmatrix}\n",
      " -4.00 & -10.00\\\\\n",
      " -4.00 & -10.00\n",
      "\\end{bmatrix}\n",
      "tensor(14.5000, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "relu dL/dW2\n",
      " tensor([[ -4.0000, -10.0000],\n",
      "        [ -4.0000, -10.0000]], dtype=torch.float64) \n",
      "\n",
      "mse=14.499999999999998\n",
      "dL/db1 \\begin{bmatrix}\n",
      "  13.00 & -1.00\n",
      "\\end{bmatrix}\n",
      "tensor(14.5000, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "relu dL/db1\n",
      " tensor([[13.0000, -1.0000]], dtype=torch.float64) \n",
      "\n",
      "(3, 1) (2, 3) (2, 1) (2, 2) (2, 1) (2, 1)\n",
      "mse=14.499999999999998\n",
      "dL/db2 \\begin{bmatrix}\n",
      " -2.00 & -5.00\n",
      "\\end{bmatrix}\n",
      "dL/dx3 = -(t - x3)^T=\n",
      " \\begin{bmatrix}\n",
      " -2.00 & -5.00\n",
      "\\end{bmatrix}\n",
      "dx3/dW2=x2^T kron I_2x2=\n",
      " \\begin{bmatrix}\n",
      "  4.00 &  4.00\n",
      "\\end{bmatrix}\n",
      "d(z)/dW1=d(W1x1+b1)/dW1=x1^T kron I_2x2=\n",
      " \\begin{bmatrix}\n",
      "  0.00 &  2.00 &  4.00\n",
      "\\end{bmatrix}\n",
      "dx3/dx2\n",
      " \\begin{bmatrix}\n",
      "  1.00 & -2.00\\\\\n",
      " -3.00 &  1.00\n",
      "\\end{bmatrix}\n",
      "dReLU(z)/dz=dReLU(W1x1+b1)/dW1=\n",
      " \\begin{bmatrix}\n",
      "  1.00 &  0.00\\\\\n",
      "  0.00 &  1.00\n",
      "\\end{bmatrix}\n",
      "z=W1x1+b1\n",
      " \\begin{bmatrix}\n",
      "  2.00\\\\\n",
      "  2.00\n",
      "\\end{bmatrix}\n",
      "x2=ReLU(z)=\n",
      " \\begin{bmatrix}\n",
      "  2.00\\\\\n",
      "  2.00\n",
      "\\end{bmatrix}\n",
      "x3=\n",
      " \\begin{bmatrix}\n",
      " -1.00\\\\\n",
      " -3.00\n",
      "\\end{bmatrix}\n",
      "tensor(14.5000, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "relu dL/db2\n",
      " tensor([[-2.0000, -5.0000]], dtype=torch.float64) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.functional as F\n",
    "import array_to_latex as a2l\n",
    "\n",
    "'''Solving Q3 using the chain rule and PyTorch, good for sigmoid and ReLU'''\n",
    "\n",
    "def my_relu(vec):\n",
    "    return np.array([max(0, v) for v in vec.flatten()]).reshape(-1, 1)\n",
    "\n",
    "def a2l_short(a):  # prints out nice latex\n",
    "    return a2l.to_ltx(a, frmt='{:.2f}', arraytype='bmatrix', print_out=False)\n",
    "\n",
    "def my_relu_derivative(vec):\n",
    "    ret = np.zeros((len(vec), len(vec)))  # square matrix\n",
    "    relu_vec = my_relu(vec)\n",
    "    # if ReLu(vec_i) > 0 then ret_{i,i} = 1, ret_{i,i} = 0 otherwise\n",
    "    relu_vec[relu_vec > 0] = 1\n",
    "    relu_vec[relu_vec <= 0] = 0\n",
    "    np.fill_diagonal(a=ret, val=relu_vec)\n",
    "    return ret\n",
    "\n",
    "def my_sigmoid_derivative(vec):  # computes diag(vec)diag(1-vec)\n",
    "    ret1 = np.zeros((len(vec), len(vec)))\n",
    "    ret2 = np.zeros((len(vec), len(vec)))\n",
    "    np.fill_diagonal(a=ret1, val=vec)\n",
    "    np.fill_diagonal(a=ret2, val=1-vec)\n",
    "    return np.matmul(ret1, ret2)\n",
    "\n",
    "def my_run(relu_or_sigmoid, wrt, verbose=False, rand=False):\n",
    "    if not rand:\n",
    "        x1 = np.array([0., 1., 2.]).reshape(-1, 1)\n",
    "        W1 = np.array([[ 3., -1,  1],\n",
    "                       [-5.,  2, -1]])\n",
    "        b1 = np.array([1., 2]).reshape(-1, 1)\n",
    "\n",
    "        W2 = np.array([[ 1., -2],\n",
    "                       [-3.,  1]])\n",
    "        b2 = np.array([1., 1]).reshape(-1, 1)\n",
    "        t = np.array([1., 2]).reshape(-1, 1)\n",
    "    else:\n",
    "        shape1 = np.random.randint(2, 12)\n",
    "        shape2 = np.random.randint(2, 12)\n",
    "        x1 = np.random.rand(1, shape1).reshape(-1, 1) * np.random.randint(low=-5, high=5)\n",
    "        W1 = np.random.rand(shape2, shape1) * np.random.randint(low=-5, high=5)\n",
    "        b1 = np.random.rand(1, shape2).reshape(-1, 1) * np.random.randint(low=-5, high=5)\n",
    "\n",
    "        shape3 = np.random.randint(2, 12)\n",
    "        W2 = np.random.rand(shape3, shape2) * np.random.randint(low=-5, high=5)\n",
    "        b2 = np.random.rand(1, shape3).reshape(-1, 1) * np.random.randint(low=-5, high=5)\n",
    "        t = np.random.rand(1, shape3).reshape(-1, 1) * np.random.randint(low=-5, high=5)\n",
    "    if verbose:\n",
    "        print(x1.shape, W1.shape, b1.shape, W2.shape, b2.shape, t.shape)\n",
    "    if relu_or_sigmoid == 'relu':\n",
    "        # print('here', (np.matmul(W1, x1) + b1).shape)\n",
    "        x2 = my_relu(np.matmul(W1, x1) + b1)\n",
    "    elif relu_or_sigmoid == 'sigmoid':\n",
    "        x2 = 1/(1 + np.exp(-(np.matmul(W1, x1) + b1)))\n",
    "    else:\n",
    "        raise KeyError('relu or sigmoid')\n",
    "    x3 = np.matmul(W2, x2) + b2\n",
    "\n",
    "    mse_here = 0.5 * np.linalg.norm(t - x3) ** 2\n",
    "    print('mse=' + str(mse_here))\n",
    "\n",
    "    if relu_or_sigmoid == 'relu':\n",
    "        step_f_x2_wrt_z = my_relu_derivative(x2)  # = diag(v)\n",
    "        # dReLU(z)/dz as matrix\n",
    "    if relu_or_sigmoid == 'sigmoid':\n",
    "        step_f_x2_wrt_z = my_sigmoid_derivative(x2)\n",
    "        # dSigma(z)/dz as matrix\n",
    "    if wrt == 'W1':  # see equation 23\n",
    "        dL_dW1 = np.matmul(np.matmul(-(t - x3).T, W2),\n",
    "                           np.matmul(step_f_x2_wrt_z, np.kron(x1.T, np.identity(W1.shape[0]))))\n",
    "        print('dL/d' + wrt + ', res before re-shaping\\n', a2l_short(dL_dW1))\n",
    "        print('res after re-shaping\\n', a2l_short(dL_dW1.reshape(W1.shape[1], W1.shape[0])))\n",
    "    if wrt == 'W2':  # see equation 24\n",
    "        print('dL/d' + wrt + ', res before re-shaping\\n',\n",
    "              a2l_short(np.matmul(-(t - x3).T, np.kron(x2.T, np.identity(W2.shape[0])))))\n",
    "        print('res after re-shaping\\n',\n",
    "              a2l_short(np.matmul(-(t - x3).T,\n",
    "                        np.kron(x2.T, np.identity(W2.shape[0]))).reshape(W2.shape[1], W2.shape[0])))\n",
    "    if wrt == 'b1':  # see equation 25\n",
    "        print('dL/d' + wrt, a2l_short(np.matmul(np.matmul(-(t - x3).T, W2), step_f_x2_wrt_z)))\n",
    "    if wrt == 'b2':  # see equation 26\n",
    "        print('dL/d' + wrt, a2l_short(-(t - x3).T))\n",
    "\n",
    "    if verbose:  # prints out intermediate steps\n",
    "        print('dL/dx3 = -(t - x3)^T=\\n', a2l_short(-(t - x3).T))\n",
    "        print('dx3/dW2=x2^T kron I_2x2=\\n', a2l_short(np.kron(x2.T, W2.shape[0])))\n",
    "        print('d(z)/dW1=d(W1x1+b1)/dW1=x1^T kron I_2x2=\\n', a2l_short(np.kron(x1.T, W1.shape[0])))\n",
    "        print('dx3/dx2\\n', a2l_short(W2))\n",
    "        print('dReLU(z)/dz=dReLU(W1x1+b1)/dW1=\\n', a2l_short(step_f_x2_wrt_z))\n",
    "        print('z=W1x1+b1\\n', a2l_short(np.matmul(W1, x1) + b1))\n",
    "        print('x2=ReLU(z)=\\n', a2l_short(x2))\n",
    "        print('x3=\\n', a2l_short(x3))\n",
    "    return x1, W1, b1, W2, b2, t\n",
    "\n",
    "\n",
    "def compute_grad_pytorch(relu_or_sigmoid, wrt,\n",
    "                         x1=None, W1=None, b1=None, W2=None, b2=None, t=None,\n",
    "                         rand=False):\n",
    "    if not rand:  # uses whatever is given in Q3.2\n",
    "        x1_ = np.array([0., 1., 2.]).reshape(-1, 1)\n",
    "        b1_ = np.array([1., 2]).reshape(-1, 1)\n",
    "        W1_ = np.array([[ 3., -1,  1],\n",
    "                       [-5.,  2, -1]])\n",
    "\n",
    "        b2_ = np.array([1., 1]).reshape(-1, 1)\n",
    "        W2_ = np.array([[ 1., -2],\n",
    "                       [-3.,  1]])\n",
    "        t_ = np.array([1., 2]).reshape(-1, 1)\n",
    "    else:  # these vars come from my_run\n",
    "        x1_ = x1\n",
    "        W1_ = W1\n",
    "        b1_ = b1\n",
    "\n",
    "        W2_ = W2\n",
    "        b2_ = b2\n",
    "        t_ = t\n",
    "\n",
    "    x1n = torch.tensor(x1_, requires_grad=True)\n",
    "    b1n = torch.tensor(b1_, requires_grad=True)\n",
    "    b2n = torch.tensor(b2_, requires_grad=True)\n",
    "    W1n = torch.tensor(W1_, requires_grad=True)\n",
    "    W2n = torch.tensor(W2_, requires_grad=True)\n",
    "\n",
    "    x1_after = torch.matmul(W1n, x1n) + b1n\n",
    "\n",
    "    if relu_or_sigmoid == 'relu':\n",
    "        x2n = torch.relu(x1_after)\n",
    "    else:\n",
    "        x2n = torch.sigmoid(x1_after)\n",
    "    x3n = torch.matmul(W2n, x2n) + b2n\n",
    "    tn = torch.tensor(t_, requires_grad=True)\n",
    "    mse = 0.5 * F.norm(tn - x3n)**2\n",
    "    print(mse)\n",
    "    which_param = {'W1': W1n, 'W2': W2n, 'b1': b1n, 'b2': b2n}\n",
    "    mse.backward(inputs=which_param[wrt])\n",
    "    print(relu_or_sigmoid + ' dL/d' + wrt + '\\n', which_param[wrt].grad.T, '\\n')\n",
    "    return\n",
    "\n",
    "\n",
    "my_run(relu_or_sigmoid='relu', wrt='W1')\n",
    "compute_grad_pytorch(relu_or_sigmoid='relu', wrt='W1')\n",
    "\n",
    "my_run(relu_or_sigmoid='relu', wrt='W2')\n",
    "compute_grad_pytorch(relu_or_sigmoid='relu', wrt='W2')\n",
    "\n",
    "my_run(relu_or_sigmoid='relu', wrt='b1')\n",
    "compute_grad_pytorch(relu_or_sigmoid='relu', wrt='b1')\n",
    "\n",
    "my_run(relu_or_sigmoid='relu', wrt='b2', verbose=True)\n",
    "compute_grad_pytorch(relu_or_sigmoid='relu', wrt='b2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-18T02:09:51.454139Z",
     "start_time": "2023-09-18T02:09:51.441866Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mse=4.201102887391705\n",
      "dL/dW1, res before re-shaping\n",
      " \\begin{bmatrix}\n",
      "  0.00 &  0.00 &  0.78 & -0.10 &  1.55 & -0.21\n",
      "\\end{bmatrix}\n",
      "res after re-shaping\n",
      " \\begin{bmatrix}\n",
      "  0.00 &  0.00\\\\\n",
      "  0.78 & -0.10\\\\\n",
      "  1.55 & -0.21\n",
      "\\end{bmatrix}\n",
      "tensor(4.2011, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "sigmoid dL/dW1\n",
      " tensor([[ 0.0000,  0.0000],\n",
      "        [ 0.7774, -0.1050],\n",
      "        [ 1.5547, -0.2100]], dtype=torch.float64) \n",
      "\n",
      "mse=4.201102887391705\n",
      "dL/dW2, res before re-shaping\n",
      " \\begin{bmatrix}\n",
      " -0.78 & -2.43 & -0.78 & -2.43\n",
      "\\end{bmatrix}\n",
      "res after re-shaping\n",
      " \\begin{bmatrix}\n",
      " -0.78 & -2.43\\\\\n",
      " -0.78 & -2.43\n",
      "\\end{bmatrix}\n",
      "tensor(4.2011, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "sigmoid dL/dW2\n",
      " tensor([[-0.7758, -2.4324],\n",
      "        [-0.7758, -2.4324]], dtype=torch.float64) \n",
      "\n",
      "mse=4.201102887391705\n",
      "dL/db1 \\begin{bmatrix}\n",
      "  0.78 & -0.10\n",
      "\\end{bmatrix}\n",
      "tensor(4.2011, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "sigmoid dL/db1\n",
      " tensor([[ 0.7774, -0.1050]], dtype=torch.float64) \n",
      "\n",
      "mse=4.201102887391705\n",
      "dL/db2 \\begin{bmatrix}\n",
      " -0.88 & -2.76\n",
      "\\end{bmatrix}\n",
      "tensor(4.2011, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "sigmoid dL/db2\n",
      " tensor([[-0.8808, -2.7616]], dtype=torch.float64) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "my_run(relu_or_sigmoid='sigmoid', wrt='W1')\n",
    "compute_grad_pytorch(relu_or_sigmoid='sigmoid', wrt='W1')\n",
    "\n",
    "my_run(relu_or_sigmoid='sigmoid', wrt='W2')\n",
    "compute_grad_pytorch(relu_or_sigmoid='sigmoid', wrt='W2')\n",
    "\n",
    "my_run(relu_or_sigmoid='sigmoid', wrt='b1')\n",
    "compute_grad_pytorch(relu_or_sigmoid='sigmoid', wrt='b1')\n",
    "\n",
    "my_run(relu_or_sigmoid='sigmoid', wrt='b2')\n",
    "compute_grad_pytorch(relu_or_sigmoid='sigmoid', wrt='b2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-18T02:09:51.487792Z",
     "start_time": "2023-09-18T02:09:51.450763Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mse=4284.1927808565515\n",
      "dL/dW1, res before re-shaping\n",
      " \\begin{bmatrix}\n",
      " -59.28 & -75.72 & -209.31 & -290.40 & -370.93 & -1025.27 & -358.42 & -457.81 & -1265.43 & -80.43 & -102.73 & -283.96 & -430.61 & -550.02 & -1520.29 & -321.76 & -410.99 & -1136.00\n",
      "\\end{bmatrix}\n",
      "res after re-shaping\n",
      " \\begin{bmatrix}\n",
      " -59.28 & -75.72 & -209.31\\\\\n",
      " -290.40 & -370.93 & -1025.27\\\\\n",
      " -358.42 & -457.81 & -1265.43\\\\\n",
      " -80.43 & -102.73 & -283.96\\\\\n",
      " -430.61 & -550.02 & -1520.29\\\\\n",
      " -321.76 & -410.99 & -1136.00\n",
      "\\end{bmatrix}\n",
      "tensor(4284.1928, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "relu dL/dW1\n",
      " tensor([[  -59.2843,   -75.7240,  -209.3059],\n",
      "        [ -290.3996,  -370.9283, -1025.2691],\n",
      "        [ -358.4225,  -457.8142, -1265.4272],\n",
      "        [  -80.4290,  -102.7323,  -283.9585],\n",
      "        [ -430.6100,  -550.0196, -1520.2886],\n",
      "        [ -321.7636,  -410.9897, -1136.0013]], dtype=torch.float64) \n",
      "\n",
      "mse=32.47118572498797\n",
      "dL/dW2, res before re-shaping\n",
      " \\begin{bmatrix}\n",
      "  0.00 &  0.00 &  0.00 &  0.00 &  0.00 &  0.00 &  0.00 &  0.00 &  0.00 &  0.00 &  0.00 &  0.00 &  0.00 &  0.00 &  0.00 &  0.00 &  0.00 &  0.00 &  0.00 &  0.00 &  0.00 &  0.00 &  0.00 &  0.00 &  0.00 &  0.00 &  0.00 &  0.00 &  0.00 &  0.00 &  0.00 &  0.00\n",
      "\\end{bmatrix}\n",
      "res after re-shaping\n",
      " \\begin{bmatrix}\n",
      "  0.00 &  0.00 &  0.00 &  0.00\\\\\n",
      "  0.00 &  0.00 &  0.00 &  0.00\\\\\n",
      "  0.00 &  0.00 &  0.00 &  0.00\\\\\n",
      "  0.00 &  0.00 &  0.00 &  0.00\\\\\n",
      "  0.00 &  0.00 &  0.00 &  0.00\\\\\n",
      "  0.00 &  0.00 &  0.00 &  0.00\\\\\n",
      "  0.00 &  0.00 &  0.00 &  0.00\\\\\n",
      "  0.00 &  0.00 &  0.00 &  0.00\n",
      "\\end{bmatrix}\n",
      "tensor(32.4712, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "relu dL/dW2\n",
      " tensor([[0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.]], dtype=torch.float64) \n",
      "\n",
      "mse=23.41035200715275\n",
      "dL/db1 \\begin{bmatrix}\n",
      "  0.00 &  0.00 &  0.00 &  0.00 &  0.00 &  0.00 &  0.00\n",
      "\\end{bmatrix}\n",
      "tensor(23.4104, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "relu dL/db1\n",
      " tensor([[0., 0., 0., 0., 0., 0., 0.]], dtype=torch.float64) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# testing with random inputs\n",
    "x1, W1, b1, W2, b2, t = my_run(relu_or_sigmoid='relu', wrt='W1', rand=True)\n",
    "compute_grad_pytorch(relu_or_sigmoid='relu', wrt='W1', rand=True,\n",
    "                     x1=x1, W1=W1, b1=b1, W2=W2, b2=b2, t=t)\n",
    "\n",
    "x1, W1, b1, W2, b2, t = my_run(relu_or_sigmoid='relu', wrt='W2', rand=True)\n",
    "compute_grad_pytorch(relu_or_sigmoid='relu', wrt='W2', rand=True,\n",
    "                     x1=x1, W1=W1, b1=b1, W2=W2, b2=b2, t=t)\n",
    "\n",
    "x1, W1, b1, W2, b2, t = my_run(relu_or_sigmoid='relu', wrt='b1', rand=True)\n",
    "compute_grad_pytorch(relu_or_sigmoid='relu', wrt='b1', rand=True,\n",
    "                     x1=x1, W1=W1, b1=b1, W2=W2, b2=b2, t=t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-18T02:09:51.488129Z",
     "start_time": "2023-09-18T02:09:51.472084Z"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
