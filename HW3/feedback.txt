1.2  wrong answer. True. The computation of attention is the same as computing cosine similarity between key and query;  -1pt. The attention mechanism of query and key consider their cosine similarity and magnitude
Your reasons are reasonable, so I will only take 1 point off; -1pt.
1.8 wrong answer. True The accumulation of large derivatives results in exploding gradients. The accumulation
of small derivatives results in vanishing gradients. Gradient clipping can avoid overflow or underflow by
limiting the range of gradients. There are two types of gradient clipping: (1) gradient clipping by value
where a minimum clip value and a maximum clip value is defined, (2) gradient clipping by norm which
ensures the gradient vector has norm at most equal to a predefined threshold; -3pt



Lab 1 (h) bonus question -- gets full points; +5pts
Lab 2 (f) bonus question -- gets full points; +5pts
Lab 2 (g) bonus question -- gets full points; +5pts