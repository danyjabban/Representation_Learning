{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from resnet import *\n",
    "from train_util import *\n",
    "from FP_layers import *\n",
    "from train_classes import *\n",
    "# from train_class import *\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "import time\n",
    "import os\n",
    "# os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"max_split_size_mb:512\"\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "# print(device, 'torch.distributed.is_available():', torch.distributed.is_available())\n",
    "print(device)\n",
    "# torch.distributed.init_process_group()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nt_xent_loss(x, temperature=0.1):\n",
    "    xcs = F.cosine_similarity(x[None, :, :], x[:, None, :], dim=-1)\n",
    "    # assume x has shape [batch_size*2, 64]\n",
    "    xcs[torch.eye(x.size(0)).bool()] = float(\"-inf\")\n",
    "\n",
    "    target = torch.arange(x.size(0))\n",
    "    target[0::2] += 1\n",
    "    target[1::2] -= 1\n",
    "\n",
    "    ce_loss = F.cross_entropy(xcs.to(device) / torch.tensor(temperature).to(device), target.to(device), reduction=\"mean\")\n",
    "    # Standard cross entropy loss\n",
    "    # only need adjacent pairs: [2k, 2k-1] and [2k-1, 2k] for k in range(batch_size/2),\n",
    "    # which is ensured by \"target\" (some magic happens I guess)\n",
    "    return ce_loss\n",
    "\n",
    "def add_contrastive_loss(hidden,\n",
    "                         hidden_norm=True,\n",
    "                         temperature=1.0,\n",
    "                         weights=1.0):\n",
    "    \"\"\"Compute loss for model.\n",
    "\n",
    "    Args:\n",
    "        hidden: hidden vector (`Tensor`) of shape (2 * bsz, dim).\n",
    "        hidden_norm: whether or not to use normalization on the hidden vector.\n",
    "        temperature: a `floating` number for temperature scaling.\n",
    "        tpu_context: context information for tpu.\n",
    "        weights: a weighting number or vector.\n",
    "\n",
    "    Returns:\n",
    "        A loss scalar.\n",
    "        The logits for contrastive prediction task.\n",
    "        The labels for contrastive prediction task.\n",
    "    \"\"\"\n",
    "    # Get (normalized) hidden1 and hidden2.\n",
    "    # print(hidden)\n",
    "    if hidden_norm:\n",
    "        hidden = F.normalize(hidden, p=2, dim=1)\n",
    "    batch_size = int(hidden.shape[0] / 2)\n",
    "    hidden1, hidden2 = torch.split(hidden, batch_size, dim=0)\n",
    "\n",
    "    # Gather hidden1/hidden2 across replicas and create local labels.\n",
    "    \n",
    "    hidden1_large = hidden1\n",
    "    # print(hidden1)\n",
    "    hidden2_large = hidden2\n",
    "    labels = F.one_hot(torch.arange(0, batch_size), batch_size * 2)\n",
    "    masks = F.one_hot(torch.arange(0, batch_size), batch_size)\n",
    "\n",
    "    logits_aa = torch.matmul(hidden1, hidden1_large.T) / temperature\n",
    "    logits_aa = logits_aa - masks * 1\n",
    "    logits_bb = torch.matmul(hidden2, hidden2_large.T) / temperature\n",
    "    logits_bb = logits_bb - masks * 1\n",
    "    logits_ab = torch.matmul(hidden1, hidden2_large.T) / temperature\n",
    "    logits_ba = torch.matmul(hidden2, hidden1_large.T) / temperature\n",
    "    loss_a = F.cross_entropy(\n",
    "        labels.type(torch.DoubleTensor), torch.concat([logits_ab, logits_aa], 1))\n",
    "    loss_b = F.cross_entropy(\n",
    "        labels.type(torch.DoubleTensor), torch.concat([logits_ba, logits_bb], 1))\n",
    "    loss = loss_a + loss_b\n",
    "\n",
    "    return loss, logits_ab, labels\n",
    "\n",
    "bs = 512\n",
    "t = 0.1\n",
    "size = 64\n",
    "crit = ContrastiveLoss(batch_size=bs, temperature=t)\n",
    "\n",
    "t1, t2 = torch.Tensor(bs, size), torch.Tensor(bs, size)\n",
    "t3 = torch.Tensor(bs*2, size)\n",
    "\n",
    "for i in range(bs):\n",
    "    t1[i, :] = i*(1+torch.Tensor(np.sin(np.linspace(0, size, num=size))))\n",
    "    t2[i, :] = i*(1+torch.Tensor(np.sin(np.linspace(0, size, num=size))))\n",
    "t3[::2, :] = t1\n",
    "t3[1::2, :] = t2\n",
    "\n",
    "print(crit(t1, t2), nt_xent_loss(t3, temperature=t), t1.shape, add_contrastive_loss(t3)[0])\n",
    "# print(world_size)\n",
    "# train_w_DDP(epochs=100, batch_size=batch_size, lr=0.3*batch_size/256, reg=1e-6, world_size=world_size, log_every_n=50)\n",
    "# rank, world_size, epochs, batch_size, lr, reg, head, log_every_n=50\n",
    "\n",
    "# rank, world_size, epochs, batch_size, lr, reg, head, log_every_n=50):\n",
    "# net, epochs, batch_size, lr, reg, rank, world_size, log_every_n=50\n",
    "# optimizer = LARS(model.parameters(), lr=0.1, momentum=0.9)\n",
    "# optimizer.zero_grad()\n",
    "# loss_fn(model(input), target).backward()\n",
    "# optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "# # model = nn.parallel.DistributedDataParallel(ResNetCIFAR(head_g=head))\n",
    "load_bs = 8192\n",
    "trainloader, testloader = Trainer_wo_DDP.cifar_dataloader_wo_ddp(bs=load_bs, train_for_finetune=0)\n",
    "\n",
    "bs = 4096\n",
    "epoch = 1000\n",
    "resnet_model_pth = \"./saved_models/epoch_%d_bs_%d_lr_%g_reg_1e-06.pt\" % \\\n",
    "                (epoch, bs, 0.3*bs/256)\n",
    "\n",
    "lin_eval_net = LinearEvaluation(method='lin', which_device=device, resnet_model_pth=resnet_model_pth, Nbits=None, symmetric=False).to(device)\n",
    "\n",
    "batch, labels = None, None\n",
    "for i, (b, l) in enumerate(testloader):\n",
    "    batch = b\n",
    "    labels = l\n",
    "    break\n",
    "    # print(lin_eval_net(batch.to(device))[0])\n",
    "_ = lin_eval_net(batch.to(device))\n",
    "out_dict = {i: [] for i in range(10)}\n",
    "for i in range(load_bs): \n",
    "    # out_dict[int(labels[i])].append(torch.norm(out[i]).cpu().detach().numpy())\n",
    "    out_dict[int(labels[i])].append(lin_eval_net.embedding[i, :].cpu().detach().numpy())\n",
    "\n",
    "for i in range(10):\n",
    "    out_dict[i] = PCA().fit_transform(np.array(out_dict[i]))\n",
    "\n",
    "# print(concat_dict(out_dict).shape)\n",
    "\n",
    "fig, ax = plt.subplots(1, 1)\n",
    "print(out_dict[0][0].shape)\n",
    "for i in range(10):\n",
    "    out_dict[i] = np.array(out_dict[i])\n",
    "    ax.scatter(out_dict[i][:, 0], out_dict[i][:, 1], s=1, alpha=0.4)\n",
    "plt.show()\n",
    "# ax.plot(sorted(out_dict[1]))\n",
    "# head = nn.Sequential(FP_Linear(64, 64, Nbits=None), nn.ReLU(True), FP_Linear(64, 64, Nbits=None))\n",
    "# # head = nn.Sequential(nn.Linear(64, 64), nn.ReLU(True), nn.Linear(64, 64))\n",
    "# # model = nn.DataParallel(ResNetCIFAR_mine(num_layers=50)).to(device)\n",
    "# model = ResNetCIFAR(head_g=head, num_layers=50).to(device)\n",
    "# # model = torch.nn.SyncBatchNorm.convert_sync_batchnorm(nn.DataParallel(ResNetCIFAR(head_g=head, num_layers=50), device_ids=[0, 1]).to(device))\n",
    "# # model = torch.nn.SyncBatchNorm.convert_sync_batchnorm(nn.DataParallel(ResNetCIFAR(head_g=head, num_layers=50).to(device), device_ids=[0, 1]))\n",
    "# # model = nn.DataParallel(ResNetCIFAR(head_g=head, num_layers=50)).to(device)\n",
    "# # train_no_DDP(model, epochs=1000, batch_size=batch_size, lr=0.3*batch_size/256, reg=1e-6, log_every_n=20)\n",
    "# train_nt_xet_class(model, epochs=1000, batch_size=batch_size, lr=0.3*batch_size/256, reg=1e-6, log_every_n=20)\n",
    "\n",
    "# # # b = torch.rand((64, 3, 28, 28))\n",
    "# # head = nn.Sequential(FP_Linear(64, 64, Nbits=None), nn.ReLU(True), FP_Linear(64, 64, Nbits=None))\n",
    "\n",
    "# # y = model(b)\n",
    "\n",
    "# # # xcs = F.cosine_similarity(y[None, :, :], y[:,None,:], dim=-1)\n",
    "# # # ce_loss = F.cross_entropy(xcs / temperature, target, reduction=\"mean\")\n",
    "# # # diff_sum = 0\n",
    "# # # for rr, cc in enumerate(range(128)):\n",
    "# #     # diff_sum += xcs[rr, cc] - F.cosine_similarity(y[rr, :], y[cc, :], dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# head = nn.Sequential(FP_Linear(64, 64, Nbits=None), nn.ReLU(True), FP_Linear(64, 64, Nbits=None))\n",
    "# model = ResNetCIFAR(head_g=head).to(device)\n",
    "# trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=False, transform=transforms.ToTensor())\n",
    "# trainloader = torch.utils.data.DataLoader(trainset, batch_size=128, shuffle=True, num_workers=1)\n",
    "# b = None\n",
    "# for idx, (inputs, label) in enumerate(trainloader):\n",
    "#     print(inputs.shape)\n",
    "#     b = augment_data(inputs)\n",
    "#     break\n",
    "T_max = 1000\n",
    "model = ResNetCIFAR()\n",
    "optimizer = LARS(model.parameters(), lr=4.8, momentum=0.9, weight_decay=1e-6, nesterov=False)\n",
    "warmup_iters = 10\n",
    "scheduler_warmup = optim.lr_scheduler.LinearLR(optimizer, start_factor=0.1, end_factor=1.0, \n",
    "                                                            total_iters=warmup_iters, verbose=False)\n",
    "scheduler_after  = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=T_max, verbose=False)\n",
    "\n",
    "lr_list = []\n",
    "for i in range(T_max):\n",
    "    if i > 10:\n",
    "        scheduler = scheduler_after\n",
    "    else:\n",
    "        scheduler = scheduler_warmup\n",
    "    lr_list.append(scheduler.get_lr())\n",
    "    scheduler.step()\n",
    "plt.plot(lr_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# idx = -2\n",
    "# plt.imshow(b[idx, 0, :, :])\n",
    "# plt.show()\n",
    "# plt.imshow(b[idx+1, 0, :, :])\n",
    "# plt.show()\n",
    "crit = ContrastiveLoss(16)\n",
    "# data_1 = torch.zeros((16, 3))\n",
    "# for i in range(8):\n",
    "#     data_1[2*i, :] = i\n",
    "#     data_1[2*i+1, :] = i\n",
    "\n",
    "data_1 = torch.zeros((16, 3))\n",
    "data_2 = torch.zeros((16, 3))\n",
    "for i in range(16):\n",
    "    data_1[i, :] = np.sqrt(i)\n",
    "    # data_2[i, :] = torch.exp(torch.arange(3))\n",
    "    # data_2[i, :] = torch.arange(3)\n",
    "    # data_2[i, :] = i\n",
    "    data_2[i, :] = 0\n",
    "crit(data_1, data_2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "michael_pytorch",
   "language": "python",
   "name": "michael_pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
