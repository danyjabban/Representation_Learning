{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "from resnet import ResNetCIFAR\n",
    "from train_util import *\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import time\n",
    "\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "import torch.multiprocessing as mp\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "from torch.distributed import init_process_group, destroy_process_group\n",
    "\n",
    "from FP_layers import *\n",
    "import os\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"max_split_size_mb:512\"\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "# print(device, 'torch.distributed.is_available():', torch.distributed.is_available())\n",
    "print(device)\n",
    "# torch.distributed.init_process_group()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    },
    {
     "ename": "ProcessRaisedException",
     "evalue": "\n\n-- Process 0 terminated with the following error:\nTraceback (most recent call last):\n  File \"/home/zl310/anaconda3/envs/michael/lib/python3.8/site-packages/torch/multiprocessing/spawn.py\", line 69, in _wrap\n    fn(i, *args)\n  File \"/home/zl310/ece661-hw-fa23/project_test/train_util.py\", line 75, in train\n    net = DDP(ResNetCIFAR(head_g=head), device_ids=[rank])\n  File \"/home/zl310/anaconda3/envs/michael/lib/python3.8/site-packages/torch/nn/parallel/distributed.py\", line 603, in __init__\n    self._log_and_throw(\n  File \"/home/zl310/anaconda3/envs/michael/lib/python3.8/site-packages/torch/nn/parallel/distributed.py\", line 769, in _log_and_throw\n    raise err_type(err_msg)\nValueError: DistributedDataParallel device_ids and output_device arguments only work with single-device/multiple-device GPU modules or CPU modules, but got device_ids [0], output_device None, and module parameters {device(type='cpu')}.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mProcessRaisedException\u001b[0m                    Traceback (most recent call last)",
      "\u001b[1;32m/home/zl310/ece661-hw-fa23/project_test/test_lars.ipynb Cell 2\u001b[0m line \u001b[0;36m1\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Btc319-srv1.egr.duke.edu/home/zl310/ece661-hw-fa23/project_test/test_lars.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=7'>8</a>\u001b[0m world_size \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcuda\u001b[39m.\u001b[39mdevice_count()\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Btc319-srv1.egr.duke.edu/home/zl310/ece661-hw-fa23/project_test/test_lars.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39mprint\u001b[39m(world_size)\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Btc319-srv1.egr.duke.edu/home/zl310/ece661-hw-fa23/project_test/test_lars.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=9'>10</a>\u001b[0m mp\u001b[39m.\u001b[39;49mspawn(train, args\u001b[39m=\u001b[39;49m(world_size, \u001b[39m100\u001b[39;49m, batch_size, \u001b[39m0.3\u001b[39;49m\u001b[39m*\u001b[39;49mbatch_size\u001b[39m/\u001b[39;49m\u001b[39m256\u001b[39;49m, \u001b[39m1e-6\u001b[39;49m, head, \u001b[39m50\u001b[39;49m,), nprocs\u001b[39m=\u001b[39;49mworld_size)\n",
      "File \u001b[0;32m~/anaconda3/envs/michael/lib/python3.8/site-packages/torch/multiprocessing/spawn.py:239\u001b[0m, in \u001b[0;36mspawn\u001b[0;34m(fn, args, nprocs, join, daemon, start_method)\u001b[0m\n\u001b[1;32m    235\u001b[0m     msg \u001b[39m=\u001b[39m (\u001b[39m'\u001b[39m\u001b[39mThis method only supports start_method=spawn (got: \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m).\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m'\u001b[39m\n\u001b[1;32m    236\u001b[0m            \u001b[39m'\u001b[39m\u001b[39mTo use a different start_method use:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\t\u001b[39;00m\u001b[39m\\t\u001b[39;00m\u001b[39m'\u001b[39m\n\u001b[1;32m    237\u001b[0m            \u001b[39m'\u001b[39m\u001b[39m torch.multiprocessing.start_processes(...)\u001b[39m\u001b[39m'\u001b[39m \u001b[39m%\u001b[39m start_method)\n\u001b[1;32m    238\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(msg)\n\u001b[0;32m--> 239\u001b[0m \u001b[39mreturn\u001b[39;00m start_processes(fn, args, nprocs, join, daemon, start_method\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mspawn\u001b[39;49m\u001b[39m'\u001b[39;49m)\n",
      "File \u001b[0;32m~/anaconda3/envs/michael/lib/python3.8/site-packages/torch/multiprocessing/spawn.py:197\u001b[0m, in \u001b[0;36mstart_processes\u001b[0;34m(fn, args, nprocs, join, daemon, start_method)\u001b[0m\n\u001b[1;32m    194\u001b[0m     \u001b[39mreturn\u001b[39;00m context\n\u001b[1;32m    196\u001b[0m \u001b[39m# Loop on join until it returns True or raises an exception.\u001b[39;00m\n\u001b[0;32m--> 197\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mnot\u001b[39;00m context\u001b[39m.\u001b[39;49mjoin():\n\u001b[1;32m    198\u001b[0m     \u001b[39mpass\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/michael/lib/python3.8/site-packages/torch/multiprocessing/spawn.py:160\u001b[0m, in \u001b[0;36mProcessContext.join\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    158\u001b[0m msg \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m-- Process \u001b[39m\u001b[39m%d\u001b[39;00m\u001b[39m terminated with the following error:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m error_index\n\u001b[1;32m    159\u001b[0m msg \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m original_trace\n\u001b[0;32m--> 160\u001b[0m \u001b[39mraise\u001b[39;00m ProcessRaisedException(msg, error_index, failed_process\u001b[39m.\u001b[39mpid)\n",
      "\u001b[0;31mProcessRaisedException\u001b[0m: \n\n-- Process 0 terminated with the following error:\nTraceback (most recent call last):\n  File \"/home/zl310/anaconda3/envs/michael/lib/python3.8/site-packages/torch/multiprocessing/spawn.py\", line 69, in _wrap\n    fn(i, *args)\n  File \"/home/zl310/ece661-hw-fa23/project_test/train_util.py\", line 75, in train\n    net = DDP(ResNetCIFAR(head_g=head), device_ids=[rank])\n  File \"/home/zl310/anaconda3/envs/michael/lib/python3.8/site-packages/torch/nn/parallel/distributed.py\", line 603, in __init__\n    self._log_and_throw(\n  File \"/home/zl310/anaconda3/envs/michael/lib/python3.8/site-packages/torch/nn/parallel/distributed.py\", line 769, in _log_and_throw\n    raise err_type(err_msg)\nValueError: DistributedDataParallel device_ids and output_device arguments only work with single-device/multiple-device GPU modules or CPU modules, but got device_ids [0], output_device None, and module parameters {device(type='cpu')}.\n"
     ]
    }
   ],
   "source": [
    "# from lars.lars import LARS\n",
    "head = nn.Sequential(FP_Linear(64, 64, Nbits=None), nn.ReLU(True), FP_Linear(64, 64, Nbits=None))\n",
    "\n",
    "# model = nn.parallel.DistributedDataParallel(ResNetCIFAR(head_g=head))\n",
    "# model = nn.DataParallel(ResNetCIFAR(head_g=head))\n",
    "\n",
    "batch_size = int(4096)\n",
    "world_size = torch.cuda.device_count()\n",
    "print(world_size)\n",
    "mp.spawn(train, args=(world_size, 100, batch_size, 0.3*batch_size/256, 1e-6, head, 50,), nprocs=world_size)\n",
    "# rank, world_size, epochs, batch_size, lr, reg, head, log_every_n=50):\n",
    "# train(model, epochs=100, batch_size=batch_size, lr=0.3*batch_size/256, reg=1e-6, world_size=world_size, log_every_n=50)\n",
    "# net, epochs, batch_size, lr, reg, rank, world_size, log_every_n=50\n",
    "# optimizer = LARS(model.parameters(), lr=0.1, momentum=0.9)\n",
    "# optimizer.zero_grad()\n",
    "# loss_fn(model(input), target).backward()\n",
    "# optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 3, 32, 32])\n"
     ]
    }
   ],
   "source": [
    "# # b = torch.rand((64, 3, 28, 28))\n",
    "# head = nn.Sequential(FP_Linear(64, 64, Nbits=None), nn.ReLU(True), FP_Linear(64, 64, Nbits=None))\n",
    "\n",
    "# model = ResNetCIFAR(head_g=head).to(device)\n",
    "# trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=False, transform=transforms.ToTensor())\n",
    "# trainloader = torch.utils.data.DataLoader(trainset, batch_size=128, shuffle=True, num_workers=1)\n",
    "# b = None\n",
    "# for idx, (inputs, label) in enumerate(trainloader):\n",
    "#     print(inputs.shape)\n",
    "#     b = inputs.to(device)\n",
    "#     break\n",
    "\n",
    "# y = model(b)\n",
    "\n",
    "# # xcs = F.cosine_similarity(y[None, :, :], y[:,None,:], dim=-1)\n",
    "# # ce_loss = F.cross_entropy(xcs / temperature, target, reduction=\"mean\")\n",
    "# # diff_sum = 0\n",
    "# # for rr, cc in enumerate(range(128)):\n",
    "#     # diff_sum += xcs[rr, cc] - F.cosine_similarity(y[rr, :], y[cc, :], dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 0, 3, 2, 5, 4, 7, 6])\n"
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "michael_pytorch",
   "language": "python",
   "name": "michael_pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
