{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "from resnet import *\n",
    "from train_util import *\n",
    "from FP_layers import *\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "import torch.multiprocessing as mp\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "from torch.distributed import init_process_group, destroy_process_group\n",
    "\n",
    "import time\n",
    "import os\n",
    "# os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"max_split_size_mb:512\"\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "# print(device, 'torch.distributed.is_available():', torch.distributed.is_available())\n",
    "print(device)\n",
    "# torch.distributed.init_process_group()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from lars.lars import LARS\n",
    "# head = nn.Sequential(FP_Linear(64, 64, Nbits=None), nn.ReLU(True), FP_Linear(64, 64, Nbits=None))\n",
    "\n",
    "# model = nn.parallel.DistributedDataParallel(ResNetCIFAR(head_g=head))\n",
    "# model = nn.DataParallel(ResNetCIFAR(head_g=head))\n",
    "\n",
    "batch_size = int(1024)\n",
    "# world_size = torch.cuda.device_count()\n",
    "# print(world_size)\n",
    "# mp.spawn(train, args=(world_size, 100, batch_size, 0.3*batch_size/256, 1e-6, head, 50,), nprocs=world_size, join='True')\n",
    "\n",
    "\n",
    "# rank, world_size, epochs, batch_size, lr, reg, head, log_every_n=50):\n",
    "# train(model, epochs=100, batch_size=batch_size, lr=0.3*batch_size/256, reg=1e-6, world_size=world_size, log_every_n=50)\n",
    "# net, epochs, batch_size, lr, reg, rank, world_size, log_every_n=50\n",
    "# optimizer = LARS(model.parameters(), lr=0.1, momentum=0.9)\n",
    "# optimizer.zero_grad()\n",
    "# loss_fn(model(input), target).backward()\n",
    "# optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() missing 1 required positional argument: 'head_g'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/zl310/ece661-hw-fa23/project_test/test_lars.ipynb Cell 3\u001b[0m line \u001b[0;36m7\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Btc319-srv1.egr.duke.edu/home/zl310/ece661-hw-fa23/project_test/test_lars.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m head \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mSequential(nn\u001b[39m.\u001b[39mLinear(\u001b[39m64\u001b[39m, \u001b[39m64\u001b[39m), nn\u001b[39m.\u001b[39mReLU(\u001b[39mTrue\u001b[39;00m), nn\u001b[39m.\u001b[39mLinear(\u001b[39m64\u001b[39m, \u001b[39m64\u001b[39m))\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Btc319-srv1.egr.duke.edu/home/zl310/ece661-hw-fa23/project_test/test_lars.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39m# model = nn.DataParallel(ResNetCIFAR_mine(num_layers=50)).to(device)\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Btc319-srv1.egr.duke.edu/home/zl310/ece661-hw-fa23/project_test/test_lars.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m model \u001b[39m=\u001b[39m ResNetCIFAR(num_layers\u001b[39m=\u001b[39;49m\u001b[39m50\u001b[39;49m)\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Btc319-srv1.egr.duke.edu/home/zl310/ece661-hw-fa23/project_test/test_lars.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39m# train_no_DDP(model, epochs=1000, batch_size=batch_size, lr=0.3*batch_size/256, reg=1e-6, log_every_n=20)\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Btc319-srv1.egr.duke.edu/home/zl310/ece661-hw-fa23/project_test/test_lars.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=8'>9</a>\u001b[0m train_nt_xet_class(model, epochs\u001b[39m=\u001b[39m\u001b[39m1000\u001b[39m, batch_size\u001b[39m=\u001b[39mbatch_size, lr\u001b[39m=\u001b[39m\u001b[39m0.3\u001b[39m\u001b[39m*\u001b[39mbatch_size\u001b[39m/\u001b[39m\u001b[39m256\u001b[39m, reg\u001b[39m=\u001b[39m\u001b[39m1e-6\u001b[39m, log_every_n\u001b[39m=\u001b[39m\u001b[39m20\u001b[39m)\n",
      "\u001b[0;31mTypeError\u001b[0m: __init__() missing 1 required positional argument: 'head_g'"
     ]
    }
   ],
   "source": [
    "# model = nn.parallel.DistributedDataParallel(ResNetCIFAR(head_g=head))\n",
    "\n",
    "\n",
    "# head = nn.Sequential(FP_Linear(64, 64, Nbits=None), nn.ReLU(True), FP_Linear(64, 64, Nbits=None))\n",
    "head = nn.Sequential(nn.Linear(64, 64), nn.ReLU(True), nn.Linear(64, 64))\n",
    "# model = nn.DataParallel(ResNetCIFAR_mine(num_layers=50)).to(device)\n",
    "model = ResNetCIFAR(head_g=head, num_layers=50).to(device)\n",
    "# train_no_DDP(model, epochs=1000, batch_size=batch_size, lr=0.3*batch_size/256, reg=1e-6, log_every_n=20)\n",
    "train_nt_xet_class(model, epochs=1000, batch_size=batch_size, lr=0.3*batch_size/256, reg=1e-6, log_every_n=20)\n",
    "\n",
    "# # b = torch.rand((64, 3, 28, 28))\n",
    "# head = nn.Sequential(FP_Linear(64, 64, Nbits=None), nn.ReLU(True), FP_Linear(64, 64, Nbits=None))\n",
    "\n",
    "# y = model(b)\n",
    "\n",
    "# # xcs = F.cosine_similarity(y[None, :, :], y[:,None,:], dim=-1)\n",
    "# # ce_loss = F.cross_entropy(xcs / temperature, target, reduction=\"mean\")\n",
    "# # diff_sum = 0\n",
    "# # for rr, cc in enumerate(range(128)):\n",
    "#     # diff_sum += xcs[rr, cc] - F.cosine_similarity(y[rr, :], y[cc, :], dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# head = nn.Sequential(FP_Linear(64, 64, Nbits=None), nn.ReLU(True), FP_Linear(64, 64, Nbits=None))\n",
    "# model = ResNetCIFAR(head_g=head).to(device)\n",
    "# trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=False, transform=transforms.ToTensor())\n",
    "# trainloader = torch.utils.data.DataLoader(trainset, batch_size=128, shuffle=True, num_workers=1)\n",
    "# b = None\n",
    "# for idx, (inputs, label) in enumerate(trainloader):\n",
    "#     print(inputs.shape)\n",
    "#     b = augment_data(inputs)\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# idx = -2\n",
    "# plt.imshow(b[idx, 0, :, :])\n",
    "# plt.show()\n",
    "# plt.imshow(b[idx+1, 0, :, :])\n",
    "# plt.show()\n",
    "crit = ContrastiveLoss(16)\n",
    "# data_1 = torch.zeros((16, 3))\n",
    "# for i in range(8):\n",
    "#     data_1[2*i, :] = i\n",
    "#     data_1[2*i+1, :] = i\n",
    "\n",
    "data_1 = torch.zeros((16, 3))\n",
    "data_2 = torch.zeros((16, 3))\n",
    "for i in range(16):\n",
    "    data_1[i, :] = np.sqrt(i)\n",
    "    # data_2[i, :] = torch.exp(torch.arange(3))\n",
    "    # data_2[i, :] = torch.arange(3)\n",
    "    # data_2[i, :] = i\n",
    "    data_2[i, :] = 0\n",
    "crit(data_1, data_2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "michael_pytorch",
   "language": "python",
   "name": "michael_pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
